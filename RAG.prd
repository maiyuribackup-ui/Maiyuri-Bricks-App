
LM Research Prompt: RAG Best Practices (Gemini-First, Production-Grade)

Role & Context
You are a Principal AI Architect designing a production-grade Retrieval Augmented Generation (RAG) system.
Your task is to conduct deep, up-to-date research on RAG best practices and apply them specifically to the product described below.
Avoid generic explanations. Every recommendation must map to concrete design or implementation changes.

Product Context (Must Be Used)

We are building an AI-powered Lead Intelligence & Coaching Platform for a construction materials business (Maiyuri Bricks).

Core flow:

Sales calls are recorded (Tamil language, real conversations).

Audio is uploaded manually.

Gemini STT generates transcripts.

AI summarizes calls, extracts:

Lead questions

Objections

Intent signals

Action items

These questions are automatically added to a growing knowledge base.

RAG is used to:

Suggest answers during future calls

Coach sales staff

Improve lead scoring

Surface trends and insights

Output must be:

Explainable

Trustworthy

Updatable over time

Gemini is used for:

STT

Embeddings

Retrieval

Reasoning (Claude is used only for coding and orchestration)

Research Objectives (Do Not Skip Any)
1. Modern RAG Architecture (2024‚Äì2025 Best Practices)

Compare:

Naive RAG

Hybrid RAG (BM25 + embeddings)

Hierarchical RAG

Agentic RAG

Self-RAG / Corrective RAG

Recommend which architecture fits this product and why

Explicitly state what NOT to use and why

2. Chunking Strategy (Critical)

Research and recommend:

Optimal chunk size for:

Conversational transcripts

Sales objections

FAQs

Semantic vs structural chunking

Overlap strategies

Speaker-aware chunking (sales rep vs lead)

How chunking impacts:

Recall

Hallucination

Token cost

üëâ Must include recommended chunk schema.

3. Embedding Strategy Using Gemini

Best Gemini embedding models (latest)

When to re-embed vs reuse

Embedding metadata design:

language

confidence score

source call

date

lead type

Dimensionality tradeoffs

Cost vs accuracy optimization

4. Knowledge Base Design

Should we separate:

Raw transcripts

Extracted questions

Validated answers

Coaching insights

Versioning strategy

How to avoid ‚Äúknowledge pollution‚Äù

How to promote human-approved answers over AI-generated ones

5. Retrieval Strategy

Top-k tuning

Hybrid search vs vector-only

Re-ranking strategies (LLM-based vs heuristic)

Context filtering rules

Freshness vs relevance tradeoff

üëâ Output must include a retrieval decision flow.

6. Prompt & Context Engineering

How to:

Prevent hallucination

Force citation from retrieved docs

Separate reasoning from output

Max context window usage (Gemini advantage)

How to structure system + developer + user prompts for RAG

7. Evaluation & Testing

RAG evaluation metrics:

Faithfulness

Answer relevance

Context recall

Offline vs online evaluation

Golden dataset creation from sales calls

Regression testing when KB updates

8. Anti-Patterns (Very Important)

Explicitly list:

Common RAG mistakes

What will break at scale

What increases hallucinations

What silently increases cost

9. What Needs to Change in Our Current Plan

Based on best practices:

What should be added

What should be removed

What should be re-designed

What can remain unchanged

This section must be blunt and opinionated.

10. Final Output Format (Strict)

Provide results in this structure:

Executive Summary (1 page)

Recommended RAG Architecture Diagram (textual)

Chunking & Embedding Spec (tables)

Retrieval Pipeline (step-by-step)

Prompting & Context Rules

Testing & Evaluation Framework

Anti-Patterns & Warnings

Concrete Changes Required for Our Product

Constraints

Assume Gemini 2.x / 2.5 capabilities

Production-grade (not demo)

Cost-aware

Multi-language (Tamil + English)

Must scale as knowledge grows

Must support explainability

Quality Bar

If your response sounds like:

‚ÄúRAG is when we retrieve documents‚Ä¶‚Äù ‚Üí ‚ùå Fail
If your response reads like:

A design review for a serious AI product ‚Üí ‚úÖ Pass